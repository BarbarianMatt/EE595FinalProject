{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /mnt/c/Users/BarbarianMatt/Code/Python/EE595ProjectAI/ns-3-dev/contrib/ai/examples/a-plus-b/use-msg-stru\n",
    "import ns3ai_apb_py_stru as py_binding\n",
    "from ns3ai_utils import Experiment\n",
    "import sys\n",
    "import traceback\n",
    "import uuid\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "from multiprocessing import Process\n",
    "import os\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!rm /dev/shm/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # msgInterface.PyRecvBegin()\n",
    "    # temp = msgInterface.GetCpp2PyStruct()\n",
    "    # C=temp.mldThptTotal\n",
    "    # print(C)\n",
    "    # msgInterface.PyRecvEnd()\n",
    "\n",
    "    # msgInterface.PySendBegin()\n",
    "    # msgInterface.GetPy2CppStruct().c = 2\n",
    "    # msgInterface.PySendEnd()\n",
    "\n",
    "    # msgInterface.PyRecvBegin()\n",
    "    # temp = msgInterface.GetCpp2PyStruct()\n",
    "    # C=temp.mldThptTotal\n",
    "    # print(C)\n",
    "    # msgInterface.PyRecvEnd()\n",
    "\n",
    "    # for j in np.arange(1):\n",
    "    #     print(\"Starting experiment...\", flush=True)\n",
    "    #     msgInterface = exp.run(setting=setting, show_output=True)\n",
    "    #     print(\"Experiment started.\", flush=True)\n",
    "    #     start_time = time.time()\n",
    "    #     try:\n",
    "    #         for i in range(1000):\n",
    "                \n",
    "                \n",
    "    #             msgInterface.PyRecvBegin()\n",
    "\n",
    "\n",
    "    #             if msgInterface.PyGetFinished() or time.time() - start_time > 20:\n",
    "    #                 break\n",
    "\n",
    "\n",
    "    #             # print(f\"Received a: {msgInterface.GetCpp2PyStruct().a}, b: {msgInterface.GetCpp2PyStruct().b}\")\n",
    "\n",
    "    #             temp = msgInterface.GetCpp2PyStruct().a + msgInterface.GetCpp2PyStruct().b\n",
    "    #             msgInterface.PyRecvEnd()\n",
    "\n",
    "    #             msgInterface.PySendBegin()\n",
    "    #             msgInterface.GetPy2CppStruct().c = temp\n",
    "    #             msgInterface.PySendEnd()\n",
    "\n",
    "    #             # print('', flush=True)\n",
    "    #             # print(i, flush=True)\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "    #         print(\"Exception occurred: {}\".format(e), flush=True)\n",
    "    #         print(\"Traceback:\", flush=True)\n",
    "    #         traceback.print_tb(exc_traceback)\n",
    "    #         exit(1)\n",
    "    #     finally:\n",
    "    #         print(\"Finally exiting...\", flush=True)\n",
    "\n",
    "    #     # del exp\n",
    "    #     del msgInterface\n",
    "    #     gc.collect()\n",
    "    #     # subprocess.run(['rm', '-f', f'/dev/shm/My_Seg_{unique_id}'])\n",
    "    #     # time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_we_care_about = ['mldThptTotal', 'acBECwminLink1','acBECwminLink2', 'mldProbLink1']\n",
    "# values_we_care_about = ['mldThptTotal', 'mldProbLink1']\n",
    "\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    \n",
    "def load_model(model, filepath):\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()\n",
    "\n",
    "SMALL_NUMBER=float('-inf')\n",
    "INPUT_DIM=len(values_we_care_about)+1\n",
    "HIDDEN_DIM=64\n",
    "\n",
    "OUTPUT_DIM=4\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, INPUT_DIM, HIDDEN_DIM,OUTPUT_DIM):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.fc1 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc4 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # self.fc5 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # self.fc6 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # self.fc7 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # self.fc8 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # self.fc9 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.move_piece = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "        A=0.07\n",
    "        self.dropout = nn.Dropout(A)\n",
    "        self.dropout1 = nn.Dropout(A)\n",
    "        self.dropout2 = nn.Dropout(A)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, y, mask,valid_moves_tensor):\n",
    "    \n",
    "       \n",
    "        x = torch.relu(self.input(y))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        # x = torch.relu(self.fc5(x))\n",
    "        # x = torch.relu(self.fc6(x))\n",
    "        # x = torch.relu(self.fc7(x))\n",
    "        # x = torch.relu(self.fc8(x))\n",
    "        # x = torch.relu(self.fc9(x))\n",
    "        \n",
    "\n",
    "        move_piece = self.move_piece(x)\n",
    "\n",
    "        if mask:\n",
    "            # mask_tensor = torch.full_like(move_piece, 0)\n",
    "            mask_tensor = torch.full_like(move_piece, SMALL_NUMBER)\n",
    "            mask_tensor[valid_moves_tensor.bool()] = 0\n",
    "\n",
    "            move_piece += mask_tensor\n",
    "        return move_piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /dev/shm/*\n",
    "\n",
    "\n",
    "def dton(state):\n",
    "    state_current = state['current_state']\n",
    "    ar1=np.array([0 if (key in state_current and (state_current[key] is None or np.isnan(state_current[key]))) else state_current[key] \n",
    "          for key in values_we_care_about if key in state_current])\n",
    "\n",
    "\n",
    "    state_previous = state['previous_state']\n",
    "    ar2=np.array([0 if (key in state_previous and (state_previous[key] is None or np.isnan(state_previous[key]))) else state_previous[key] \n",
    "          for key in ['mldProbLink1'] if key in state_previous])\n",
    "    if len(ar2) == 0:\n",
    "        ar2 = np.zeros(1)\n",
    "    # state_previous = state['previous_state']\n",
    "    # ar2=np.array([0 if (key in state_previous and (state_previous[key] is None or np.isnan(state_previous[key]))) else state_previous[key] \n",
    "    #       for key in values_we_care_about if key in state_previous])\n",
    "    # if len(ar2) < len(ar1):\n",
    "    #     ar2 = np.zeros(len(ar1))\n",
    "    # ar2=np.array([])\n",
    "\n",
    "    return np.concatenate((ar1,ar2))\n",
    "def convert(state):\n",
    "    return torch.FloatTensor(state.flatten()).unsqueeze(0)\n",
    "\n",
    "def move_nn_to_adjustments(move_nn, state, obj):\n",
    "\n",
    "\n",
    "    if move_nn == 0:\n",
    "        obj.cwStep1 -= 0.5\n",
    "        current_acBECwminLink1 = int(np.ceil(2**obj.cwStep1))\n",
    "\n",
    "        obj.cwStep2 -= 0.5\n",
    "        current_acBECwminLink2 = int(np.ceil(2**obj.cwStep2))\n",
    "\n",
    "\n",
    "    elif move_nn == 1:\n",
    "        obj.cwStep1 += 0.5\n",
    "        current_acBECwminLink1 = int(np.ceil(2**obj.cwStep1))\n",
    "\n",
    "        obj.cwStep2 -= 0.5\n",
    "        current_acBECwminLink2 = int(np.ceil(2**obj.cwStep2))\n",
    "    \n",
    "    elif move_nn == 2:\n",
    "        obj.cwStep1 -= 0.5\n",
    "        current_acBECwminLink1 = int(np.ceil(2**obj.cwStep1))\n",
    "\n",
    "        obj.cwStep2 += 0.5\n",
    "        current_acBECwminLink2 = int(np.ceil(2**obj.cwStep2))\n",
    "\n",
    "\n",
    "    elif move_nn == 3:\n",
    "        obj.cwStep1 += 0.5\n",
    "        current_acBECwminLink1 = int(np.ceil(2**obj.cwStep1))\n",
    "\n",
    "        obj.cwStep2 += 0.5\n",
    "        current_acBECwminLink2 = int(np.ceil(2**obj.cwStep2))\n",
    "\n",
    "    adjustments= {\n",
    "        'acBECwminLink1' : current_acBECwminLink1,\n",
    "        'acBECwminLink2' : current_acBECwminLink2,\n",
    "    }\n",
    "\n",
    "    return adjustments\n",
    "\n",
    "class SimpleAgent:\n",
    "    def __init__(self, filepath):\n",
    "        self.dqn=DQN(INPUT_DIM, HIDDEN_DIM,OUTPUT_DIM)\n",
    "        self.dqn.load_state_dict(torch.load(filepath))\n",
    "        self.type = 'DQN'\n",
    "\n",
    "    def decide(self, state, valid_moves):\n",
    "\n",
    "        self.dqn.eval()\n",
    "        state_c=convert(dton(state))\n",
    "        valid_moves_c=convert(valid_moves)\n",
    "        with torch.no_grad():\n",
    "            # move_piece_q  = self.dqn(state,True,convert(self.game.vector_valid_moves(self.unique_id)))\n",
    "            move_piece_q  = self.dqn(state_c,True, valid_moves_c)\n",
    "            num_moves_choices=1\n",
    "            move_nn=torch.argsort(move_piece_q[0], descending=True)[np.random.choice(num_moves_choices)].item()\n",
    "\n",
    "            return [move_nn]\n",
    "\n",
    "class Environnment:\n",
    "    def __init__(self, step_size = 10, max_steps=100, mldPerNodeLambda=1e-5):\n",
    "\n",
    "        self.step_size = step_size\n",
    "        self.max_steps=max_steps\n",
    "        self.mldPerNodeLambda= mldPerNodeLambda\n",
    "        self.done_simulation=False\n",
    "        self.end_experiment = False\n",
    "\n",
    "        self.experiments=-1\n",
    "\n",
    "        first_segment = np.linspace(0.5, 1, self.max_steps // 3, endpoint=False)\n",
    "        second_segment = np.linspace(1, 0, self.max_steps-len(first_segment), endpoint=False)\n",
    "        self.path1 = np.round(np.concatenate([first_segment, second_segment]),decimals=4)\n",
    "\n",
    "        first_segment = np.linspace(0.5, 0, self.max_steps // 3, endpoint=False)\n",
    "        second_segment = np.linspace(0, 1, self.max_steps-len(first_segment), endpoint=False)\n",
    "        self.path2 = np.round(np.concatenate([first_segment, second_segment]),decimals=4)\n",
    "\n",
    "\n",
    "        first_segment = np.ones(self.max_steps)\n",
    "        self.path3 = np.round(first_segment, decimals=4)\n",
    "\n",
    "        self.unique_id = str(uuid.uuid4())\n",
    "\n",
    "        self.filepath = '/mnt/c/Users/BarbarianMatt/Code/Python/EE595ProjectAI/ns-3-dev'\n",
    "\n",
    "        setting = { \"num_env\": 1, \n",
    "                    \"m_segmentName\":f\"My_Seg_{self.unique_id}\",\n",
    "                    \"m_cpp2pyMsgName\":f\"My_Cpp_to_Python_Msg_{self.unique_id}\",\n",
    "                    \"m_py2cppMsgName\":f\"My_Python_to_Cpp_Msg_{self.unique_id}\",\n",
    "                    \"m_lockableName\":f\"My_Lockable_{self.unique_id}\"\n",
    "                    }\n",
    "\n",
    "        self.exp = Experiment(\n",
    "            \"ns3ai_apb_msg_stru\",\n",
    "            self.filepath,\n",
    "            py_binding,\n",
    "            handleFinish=True,\n",
    "            segName=setting['m_segmentName'],\n",
    "            cpp2pyMsgName=setting['m_cpp2pyMsgName'],\n",
    "            py2cppMsgName=setting['m_py2cppMsgName'],\n",
    "            lockableName=setting['m_lockableName']\n",
    "        )\n",
    "\n",
    "        result = subprocess.run(['ls', '/dev/shm'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "        # Print the output (list of shared memory segments)\n",
    "        if result.returncode == 0:\n",
    "            print(\"Shared memory segments:\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "\n",
    "        print(\"Starting experiment...\", flush=True)\n",
    "        self.msgInterface = self.exp.run(setting=setting, show_output=True)\n",
    "        print(\"Experiment started.\", flush=True)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def sendReceive(self):\n",
    "        self.msgInterface.PySendBegin()\n",
    "        msg = self.msgInterface.GetPy2CppStruct()\n",
    "        msg.done_simulation = self.done_simulation\n",
    "        msg.end_experiment = self.end_experiment\n",
    "        for key in self.parameters:\n",
    "            if key != 'mldProbLink1':\n",
    "                val = self.parameters[key]\n",
    "            else:\n",
    "                val = self.path[self.num_steps] if self.num_steps< len(self.path) else self.path[-1]\n",
    "                self.parameters[key] = val\n",
    "            setattr(msg, key, val)\n",
    "\n",
    "        self.msgInterface.PySendEnd()\n",
    "\n",
    "        self.msgInterface.PyRecvBegin()\n",
    "        self.state_cc = self.msgInterface.GetCpp2PyStruct()\n",
    "        self.msgInterface.PyRecvEnd()\n",
    "        self.update_state()\n",
    "\n",
    "        # print(f'Test: {self.state_cc.simulationTime}')\n",
    "\n",
    "    def update_state(self):\n",
    "        # Save the current state as previous_state before updating the state\n",
    "        self.previous_state = self.state.copy() if hasattr(self, 'state') else {}\n",
    "\n",
    "        # Update the current state based on the new values from state_cc\n",
    "        d = dir(self.state_cc)\n",
    "        state = {}\n",
    "        for attribute in d:\n",
    "            if not attribute[:2] == '__':\n",
    "                state[attribute] = getattr(self.state_cc, attribute)\n",
    "        self.state = state\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.parameters = {\n",
    "            'acBECwminLink1': 16,\n",
    "            'acBECwminLink2': 16,\n",
    "            'acBECwStageLink1': 6,\n",
    "            'simulationTime': self.step_size,\n",
    "            'mldPerNodeLambda': self.mldPerNodeLambda,\n",
    "            'totalSteps': self.max_steps,\n",
    "            'mldProbLink1': 0.5,\n",
    "        }\n",
    "\n",
    "        paths = [self.path1, self.path2, self.path3]\n",
    "        self.path = paths[self.experiments % len(paths)]\n",
    "        if self.experiments % len(paths)==2:\n",
    "            self.path = self.path*np.random.rand()\n",
    "\n",
    "        self.cwStep1=4\n",
    "        self.cwStep2=4\n",
    "\n",
    "        self.num_steps=0\n",
    "\n",
    "        self.done_simulation=True \n",
    "        self.step()\n",
    "        self.done_simulation=False\n",
    "\n",
    "        self.experiments+=1\n",
    "\n",
    "        return self.get_state()\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        # if self.num_steps>= self.max_steps:\n",
    "        #     self.done_simulation=True\n",
    "        self.sendReceive()\n",
    "        self.num_steps +=1\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Return both the current state and the previous state\n",
    "        return {'current_state': self.state, 'previous_state': self.previous_state}\n",
    "    \n",
    "    def update_environment(self, move_nn):\n",
    "\n",
    "        action = move_nn_to_adjustments(move_nn, self.get_state(), self)\n",
    "\n",
    "        # # # print('action: ', action)\n",
    "\n",
    "        for key in action:\n",
    "            adjustment = action[key]\n",
    "            self.parameters[key]=adjustment\n",
    "\n",
    "        # print('parameters: ', self.parameters)\n",
    "        # print(f'{self.state[\"stepNumber\"]}-{self.experiments}', end=\" \")\n",
    "        print(f'{self.state[\"stepNumber\"]}', end=\" \")\n",
    "        \n",
    "        self.step()\n",
    "\n",
    "        reward = self.calc_reward()\n",
    "\n",
    "        \n",
    "        done = self.num_steps>self.max_steps\n",
    "\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "\n",
    "\n",
    "    def calc_reward(self):\n",
    "        reward = self.state['mldThptTotal']\n",
    "        return reward\n",
    "\n",
    "    def get_valid_moves(self):\n",
    "        valid_moves = np.array([1,1,1,1])\n",
    "\n",
    "        if self.state['acBECwminLink1'] <=1:\n",
    "            valid_moves[0]=0\n",
    "            valid_moves[2]=0\n",
    "        \n",
    "        elif self.state['acBECwminLink1'] >=1e7:\n",
    "            valid_moves[1]=0\n",
    "            valid_moves[3]=0\n",
    "\n",
    "        if self.state['acBECwminLink2'] <=1:\n",
    "            valid_moves[0]=0\n",
    "            valid_moves[1]=0\n",
    "        \n",
    "        elif self.state['acBECwminLink2'] >=1e7:\n",
    "            valid_moves[2]=0\n",
    "            valid_moves[3]=0\n",
    "\n",
    "\n",
    "        return valid_moves\n",
    "    def end(self):\n",
    "        self.end_experiment=True \n",
    "        self.sendReceive()\n",
    "        self.end_experiment=False\n",
    "\n",
    "def run():\n",
    "    x = Environnment(mldPerNodeLambda=0.01, step_size=0.1)\n",
    "    try:\n",
    "        i = 0\n",
    "        x.parameters['acBECwminLink1']=16\n",
    "        # x.parameters['acBECwStageLink1']=2\n",
    "        # print(x.get_state()['mldThptTotal'], i, flush=True)\n",
    "        for i in np.arange(1, 16):\n",
    "            # x.step()\n",
    "            x.update_environment(3)\n",
    "            # x.step()\n",
    "            # print(x.get_state()['mldThptTotal'], i, flush=True)\n",
    "            print(dton(x.get_state()))\n",
    "            # x.reset()\n",
    "            # print(x.get_state()['mldThptTotal'], i, flush=True)\n",
    "        \n",
    "        # x.step()\n",
    "        # # x.parameters['acBECwminLink1'] = 32\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "\n",
    "    finally:\n",
    "        x.end()\n",
    "\n",
    "\n",
    "# process = Process(target=run)\n",
    "# process.start()\n",
    "# process.join()\n",
    "# del process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.size=len(self.buffer)\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(args)\n",
    "        self.size=len(self.buffer)\n",
    "    # random sampling\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        indices = np.random.choice(len(self.buffer), size=batch_size, replace=False)\n",
    "        samples=[self.buffer[i] for i in indices]\n",
    "        self.buffer = deque([self.buffer[i] for i in range(len(self.buffer)) if i not in indices], maxlen=self.capacity)\n",
    "        # samples = [self.buffer.popleft() for _ in range(batch_size)]\n",
    "        self.size=len(self.buffer)\n",
    "        return samples\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done', 'valid_moves', 'next_valid_moves'))\n",
    "\n",
    "def train_dqn(env, dqn, target_dqn, episodes=1000, batch_size=64, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01,lr_start=1e-4, lr_end=1e-4, eval_every=100, games_per_eval=100):\n",
    "    episodes=int(episodes)\n",
    "    optimizer = optim.Adam(dqn.parameters(), lr_start)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    lr_decay= (lr_end / lr_start) ** (1 / episodes)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=lr_decay)\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    epsilon_decay = (epsilon_end / epsilon_start) ** (1 / (0.98*episodes))\n",
    "    optimizer.optimizations=0\n",
    "    optimizer.optimizations_cyclic=0\n",
    "\n",
    "    replay_buffer=ReplayBuffer(3*batch_size)\n",
    "\n",
    "    total_rewards_list=[]\n",
    "\n",
    "    # filepath='./dqn_new_model.pth'\n",
    "    # load_model(dqn, filepath)\n",
    "    # load_model(target_dqn, filepath)\n",
    "\n",
    "    # print(\"correct one good job\")\n",
    "    def optimize_model(replay_buffer, model, target_model,optimizer):\n",
    "        total_loss=0\n",
    "        model.train()\n",
    "        if True:\n",
    "        # while replay_buffer.size >= batch_size:\n",
    "            optimizer.optimizations+=1\n",
    "            optimizer.optimizations_cyclic+=batch_size\n",
    "            transitions = replay_buffer.sample(batch_size) # state, move, reward, resulting_state, done, repeat\n",
    "            batch = Transition(*zip(*transitions)) # seperates into a more usable structure\n",
    "\n",
    "            # print('batch: ',batch)\n",
    "            # print(batch.action)\n",
    "\n",
    "            # seperate batch into variables\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.tensor(batch.action, dtype=torch.int64) #change to int8\n",
    "            reward_batch = torch.tensor(batch.reward, dtype=torch.float32)\n",
    "            next_state_batch = torch.cat(batch.next_state)\n",
    "            done_batch = torch.tensor(batch.done, dtype=torch.float32)\n",
    "            valid_moves_batch = torch.cat(batch.valid_moves)\n",
    "            next_valid_moves_batch = torch.cat(batch.next_valid_moves)\n",
    "            # print(next_state_batch[0])\n",
    "            # print(next_state_batch[1])\n",
    "            # print()\n",
    "            \n",
    "\n",
    "            q_values = model(state_batch,True, valid_moves_batch) # the models responses to different states, masked for legality\n",
    "            state_action_values = q_values.gather(1, action_batch.view(-1, 1)).squeeze() # q_values of the actions it actually took\n",
    "\n",
    "            next_q_values = model(next_state_batch, True,next_valid_moves_batch) # the models responses to the resulting states as the next player\n",
    "            next_actions = next_q_values.max(1)[1] # what that player likely would have done\n",
    "            next_target_q_values = target_model(next_state_batch, True,next_valid_moves_batch) # target model which is updated less frequently for stability\n",
    "            next_state_values = next_target_q_values.gather(1, next_actions.unsqueeze(1)).squeeze() # q_values of the other player likely to result from your actions\n",
    "            \n",
    "            # print('reward batch: ',reward_batch)\n",
    "            # print('action batch: ',action_batch.view(-1, 1))\n",
    "            # print('next_actions: ',next_actions)\n",
    "            # print('state batch: ',state_batch)\n",
    "            # print('next state batch: ',next_state_batch)\n",
    "            # print('state_action_values: ',state_action_values)\n",
    "            # print('q_values',q_values)\n",
    "            # print('next_target_q_values',next_target_q_values)\n",
    "            # print('next_state_values',next_state_values)\n",
    "            # print('done batch',done_batch)\n",
    "            # print('valid_moves batch',valid_moves_batch)\n",
    "            # print('newt_valid_moves batch',next_valid_moves_batch)\n",
    "\n",
    "            expected_state_action_values = reward_batch + gamma * (next_state_values * (1 - done_batch)) # what the reward is from your actions\n",
    "\n",
    "            # print(expected_state_action_values)\n",
    "\n",
    "            # for param in model.parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         print(param.grad.norm().item())\n",
    "\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "\n",
    "            # for param in model.parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         print(param.grad.norm().item())\n",
    "            \n",
    "            base_loss =loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "            # wild_jack_mask = (action_batch >= 96) & (action_batch < 192)  # Identify wild jack moves\n",
    "            # proper_use_mask = reward_batch > 0  # Identify moves that resulted in a positive reward\n",
    "\n",
    "            # # If the wild jack was used but no positive reward was obtained, increase the loss\n",
    "            # wild_jack_loss_weight = torch.ones_like(base_loss)  # Initialize loss weights to 1 (no change)\n",
    "            # wild_jack_loss_weight[wild_jack_mask & ~proper_use_mask] = 1.5  # Increase loss for wasted wild jacks\n",
    "            # wild_jack_loss_weight[wild_jack_mask & proper_use_mask] = 0.8   # Decrease loss for good use of wild jacks\n",
    "\n",
    "            # # Apply weighted loss\n",
    "            # loss = base_loss * wild_jack_loss_weight\n",
    "            loss= base_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            # for param in model.parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         print(param.grad.norm().item())\n",
    "            optimizer.step()\n",
    "            total_loss+=loss.item()\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def select_epsilon_greedy_action(env, model, state, epsilon, valid_moves):\n",
    "        rand=np.random.rand()\n",
    "        # print('state: ', state, state.shape)\n",
    "        if rand<epsilon:\n",
    "\n",
    "            valid=np.where(valid_moves.numpy()[0]==1)[0]\n",
    "\n",
    "            if state.numpy()[0][3] >=0.5:\n",
    "                move =np.random.choice([1,3])\n",
    "            elif state.numpy()[0][3] <0.5:\n",
    "                move =np.random.choice([0,2])\n",
    "            # move =np.random.choice(valid)\n",
    "            if not move in list(valid):\n",
    "                move =np.random.choice(valid)\n",
    "            return [move]\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # print('Choose Move')\n",
    "                model.eval()\n",
    "                q_values = model(state,True, valid_moves)\n",
    "                # print('After Choose Move')\n",
    "                # print('valid_moves: ',q_values[0].cpu().numpy())\n",
    "                # print('valid_moves: ',np.isfinite(q_values[0].cpu().numpy()))\n",
    "                valid_num_moves=np.sum(np.array(np.isfinite(q_values[0].cpu().numpy())))\n",
    "                moves_to_pick_from=min(valid_num_moves,1)\n",
    "\n",
    "\n",
    "                if moves_to_pick_from > 0:\n",
    "                    return [torch.argsort(q, descending=True)[np.random.choice(moves_to_pick_from)].item() for q in q_values]\n",
    "                else:\n",
    "                    print(state)\n",
    "                    print(\"Error No Valid Moves\", flush=True)\n",
    "                    return []\n",
    "    \n",
    "    total_losses = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # print('episode: ', episode, flush=True)\n",
    "        state = convert(dton(env.reset()))\n",
    "        # print('Hello', flush=True)\n",
    "        valid_moves = convert(env.get_valid_moves())\n",
    "        total_rewards = 0\n",
    "        episode_q_values = []\n",
    "        done = False\n",
    "        moves=0\n",
    "        \n",
    "        while not done:\n",
    "            # print('Move Number: ', moves)\n",
    "            if sum(valid_moves[0])>0:\n",
    "                # print('Select Move')\n",
    "                action = select_epsilon_greedy_action(env, dqn, state, epsilon,valid_moves)\n",
    "                # print('Before Update')\n",
    "                next_state, reward, done = env.update_environment(action[0])\n",
    "                # print('After Update')\n",
    "                if episodes-episode<=2:\n",
    "                # if True:\n",
    "\n",
    "                    np.set_printoptions(formatter={'all': lambda x: f' {x}'},suppress=True)\n",
    "\n",
    "                    # Example usage with your variables\n",
    "                    print(np.array(action), \n",
    "                        np.array2string(np.round(np.array(dton(next_state)), decimals=4), separator=','), \n",
    "                        np.round(reward, decimals=4), \n",
    "                        done)\n",
    "                next_state=convert(dton(next_state))\n",
    "                next_valid_moves=convert(env.get_valid_moves())\n",
    "                if not done and sum(next_valid_moves[0])>0:\n",
    "                    replay_buffer.push(state, action, reward, next_state, done, valid_moves,next_valid_moves)\n",
    "                else:\n",
    "                    replay_buffer.push(state, action, reward, state, True, valid_moves,valid_moves)\n",
    "                    done=True\n",
    "                state = next_state\n",
    "                valid_moves=next_valid_moves\n",
    "                total_rewards+=reward\n",
    "                # print('total_rewards: ',total_rewards)\n",
    "                moves+=1\n",
    "            else:\n",
    "                print('no valid moves backup procedure', flush=True)\n",
    "                print(state)\n",
    "                state,action,reward,next_state,done,valid_moves,next_valid_moves=replay_buffer.buffer.popleft()\n",
    "                replay_buffer.push(state, action, 0, state, True, valid_moves,valid_moves)\n",
    "                done=True\n",
    "            # print('End of Move')\n",
    "        print(\"\", flush=True)  \n",
    "        # print('Hello 2!', flush=True)\n",
    "        losses = optimize_model(replay_buffer, dqn, target_dqn, optimizer)\n",
    "        # print('losses: ', losses)\n",
    "        if losses > 0:\n",
    "            total_losses = losses\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr=scheduler.get_last_lr()[0]\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        model_filepath=\"/mnt/c/Users/BarbarianMatt/Code/Python/EE595ProjectAI/ns-3-dev/contrib/ai/examples/a-plus-b/use-msg-stru/dqn_new_model.pth\"\n",
    "        if episode % (max(episodes//10,1)) == 0 and episode>max(episodes//10,1):\n",
    "            print(\"saved over!\", flush=True)\n",
    "            save_model(dqn, model_filepath)\n",
    "        \n",
    "        if optimizer.optimizations_cyclic > 1:\n",
    "            # print('target dqn updated')\n",
    "            target_dqn.load_state_dict(dqn.state_dict())\n",
    "            optimizer.optimizations_cyclic=0\n",
    "        \n",
    "        strin=f'Episode {episode + 1}/{episodes}, Epsilon: {epsilon:.3f}, Lr: {current_lr:.3e} '\n",
    "\n",
    "        strin+= f'Rewards: {total_rewards:.4f}, Losses: {total_losses:.3f}, '\n",
    "       \n",
    "        strin+=f'Total Moves: {moves}, '\n",
    "        strin+=f'Total Optimizations: {optimizer.optimizations}'\n",
    "        \n",
    "        print(strin, flush=True)\n",
    "\n",
    "        total_rewards_list.append(total_rewards)\n",
    "\n",
    "    # env.end()\n",
    "    print(\"Training complete, saving final model.\", flush=True)\n",
    "    save_model(dqn, model_filepath)\n",
    "\n",
    "\n",
    "\n",
    "    return total_rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /dev/shm/*\n",
    "np.random.seed(25)\n",
    "\n",
    "def run_ai():\n",
    "    env=Environnment(step_size=0.1,max_steps=60, mldPerNodeLambda=0.1)\n",
    "    try:\n",
    "\n",
    "        dqn = DQN(INPUT_DIM, HIDDEN_DIM,OUTPUT_DIM)\n",
    "        dqn_target = DQN(INPUT_DIM, HIDDEN_DIM,OUTPUT_DIM)\n",
    " \n",
    "\n",
    "        _=total_reward_lists=train_dqn(env, dqn, dqn_target, episodes=20, batch_size=60*8, gamma=0.999, epsilon_start=1, epsilon_end=0.05, lr_start=1e-2, lr_end=1e-4)\n",
    "\n",
    "        print('Awesome2')\n",
    "        # env.end()\n",
    "        # del env\n",
    "        print('Awesome')\n",
    "    finally:\n",
    "        env.end()\n",
    "        # del env\n",
    "        print('Awesome3')\n",
    "\n",
    "process = Process(target=run_ai)\n",
    "process.start()\n",
    "process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /dev/shm/*\n",
    "np.random.seed(25)\n",
    "\n",
    "def run_ai2():\n",
    "    env=Environnment(step_size=0.1,max_steps=60, mldPerNodeLambda=0.1)\n",
    "    model_filepath=\"/mnt/c/Users/BarbarianMatt/Code/Python/EE595ProjectAI/ns-3-dev/contrib/ai/examples/a-plus-b/use-msg-stru/dqn_new_model.pth\"\n",
    "    agent = SimpleAgent(model_filepath)\n",
    "\n",
    "    for loop in np.arange(3):\n",
    "\n",
    "\n",
    "        throughput=[]\n",
    "        mldlambda=[]\n",
    "        cwmin1=[]\n",
    "        cwmin2=[]\n",
    "\n",
    "        done = False\n",
    "        moves=0\n",
    "        while not done:\n",
    "            state = env.get_state()\n",
    "            state_c = dton(state)\n",
    "            action = agent.decide(state, env.get_valid_moves())\n",
    "            next_state, reward, done = env.update_environment(action[0])\n",
    "            \n",
    "\n",
    "            np.set_printoptions(formatter={'all': lambda x: f' {x}'},suppress=True)\n",
    "\n",
    "            # Example usage with your variables\n",
    "            print(np.array(action), \n",
    "                np.array2string(np.round(np.array(dton(next_state)), decimals=4), separator=','), \n",
    "                np.round(reward, decimals=4), \n",
    "                done)\n",
    "\n",
    "            throughput.append(state_c[0])\n",
    "            mldlambda.append(state_c[3])\n",
    "\n",
    "            cwmin1.append(state_c[1])\n",
    "            cwmin2.append(state_c[2])\n",
    "\n",
    "        \n",
    "        print(\"\", flush=True)\n",
    "\n",
    "\n",
    "        steps = range(len(throughput))  # This assumes all lists are of the same length\n",
    "\n",
    "        # Plot throughput on its own figure\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(steps, throughput, label=\"Throughput\", color='b', linestyle='-', marker='o')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Throughput')\n",
    "        plt.title('Throughput Over Steps')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot mldlambda on its own figure\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(steps, mldlambda, label=\"MldLambda\", color='g', linestyle='-', marker='x')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('MldLambda')\n",
    "        plt.title('MldLambda Over Steps')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot cwmin1 and cwmin2 on the same figure\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(steps, cwmin1, label=\"Cwmin1\", color='r', linestyle='-', marker='s')\n",
    "        plt.plot(steps, cwmin2, label=\"Cwmin2\", color='m', linestyle='-', marker='^')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('CW Min Values')\n",
    "        plt.title('CW Min Values Over Steps')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f'TOTAL THROUGHPUT: {np.sum(np.array(throughput))}')\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "\n",
    "process = Process(target=run_ai2)\n",
    "process.start()\n",
    "process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /dev/shm/*\n",
    "np.random.seed(25)\n",
    "\n",
    "def run_ai3():\n",
    "    env=Environnment(step_size=0.1,max_steps=60, mldPerNodeLambda=0.1)\n",
    "\n",
    "    for loop in np.arange(3):\n",
    "\n",
    "\n",
    "        throughput=[]\n",
    "        mldlambda=[]\n",
    "        cwmin1=[]\n",
    "        cwmin2=[]\n",
    "\n",
    "        done = False\n",
    "        moves=0\n",
    "        for i in np.arange(60):\n",
    "            state = env.get_state()\n",
    "            state_c = dton(state)\n",
    "            env.step()\n",
    "            \n",
    "\n",
    "            np.set_printoptions(formatter={'all': lambda x: f' {x}'},suppress=True)\n",
    "\n",
    "            if i>=env.max_steps:\n",
    "                done=True\n",
    "            \n",
    "\n",
    "            throughput.append(state_c[0])\n",
    "            mldlambda.append(state_c[3])\n",
    "\n",
    "            cwmin1.append(state_c[1])\n",
    "            cwmin2.append(state_c[2])\n",
    "\n",
    "        \n",
    "        print(\"\", flush=True)\n",
    "\n",
    "\n",
    "        steps = range(len(throughput))  # This assumes all lists are of the same length\n",
    "\n",
    "        # Plot throughput on its own figure\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(steps, throughput, label=\"Throughput\", color='b', linestyle='-', marker='o')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Throughput')\n",
    "        plt.title('Throughput Over Steps')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot mldlambda on its own figure\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(steps, mldlambda, label=\"MldLambda\", color='g', linestyle='-', marker='x')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('MldLambda')\n",
    "        plt.title('MldLambda Over Steps')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot cwmin1 and cwmin2 on the same figure\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(steps, cwmin1, label=\"Cwmin1\", color='r', linestyle='-', marker='s')\n",
    "        plt.plot(steps, cwmin2, label=\"Cwmin2\", color='m', linestyle='-', marker='^')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('CW Min Values')\n",
    "        plt.title('CW Min Values Over Steps')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f'TOTAL THROUGHPUT: {np.sum(np.array(throughput))}')\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "\n",
    "process = Process(target=run_ai3)\n",
    "process.start()\n",
    "process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm /dev/shm/*\n",
    "# def run_experiment(loop_id):\n",
    "\n",
    "#     unique_id = str(uuid.uuid4())\n",
    "#     print(f\"Starting loop {loop_id}, PID: {os.getpid()}\")\n",
    "\n",
    "#     filepath = '/mnt/c/Users/BarbarianMatt/Code/Python/EE595ProjectAI/ns-3-dev'\n",
    "\n",
    "#     setting = { \"num_env\": 1, \n",
    "#                 \"m_segmentName\":f\"My_Seg_{unique_id}\",\n",
    "#                 \"m_cpp2pyMsgName\":f\"My_Cpp_to_Python_Msg_{unique_id}\",\n",
    "#                 \"m_py2cppMsgName\":f\"My_Python_to_Cpp_Msg_{unique_id}\",\n",
    "#                 \"m_lockableName\":f\"My_Lockable_{unique_id}\"\n",
    "#                 }\n",
    "\n",
    "#     exp = Experiment(\n",
    "#         \"ns3ai_apb_msg_stru\",\n",
    "#         filepath,\n",
    "#         py_binding,\n",
    "#         handleFinish=True,\n",
    "#         segName=setting['m_segmentName'],\n",
    "#         cpp2pyMsgName=setting['m_cpp2pyMsgName'],\n",
    "#         py2cppMsgName=setting['m_py2cppMsgName'],\n",
    "#         lockableName=setting['m_lockableName']\n",
    "#     )\n",
    "\n",
    "#     result = subprocess.run(['ls', '/dev/shm'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "#     # Print the output (list of shared memory segments)\n",
    "#     if result.returncode == 0:\n",
    "#         print(\"Shared memory segments:\")\n",
    "#         print(result.stdout)\n",
    "#     else:\n",
    "#         print(f\"Error: {result.stderr}\")\n",
    "\n",
    "#     print(\"Starting experiment...\", flush=True)\n",
    "#     msgInterface = exp.run(setting=setting, show_output=True)\n",
    "#     print(\"Experiment started.\", flush=True)\n",
    "\n",
    "#     end = False\n",
    "#     for j in np.arange(10):\n",
    "#         done = False\n",
    "#         for i in np.arange(20):\n",
    "#             if i == 19:\n",
    "#                 done = True\n",
    "#             if done and j == 9:\n",
    "#                 end = True\n",
    "            \n",
    "#             msgInterface.PySendBegin()\n",
    "#             temp =msgInterface.GetPy2CppStruct()\n",
    "            \n",
    "#             temp.done_simulation = done\n",
    "#             temp.end_experiment = end\n",
    "#             msgInterface.PySendEnd()\n",
    "\n",
    "#             msgInterface.PyRecvBegin()\n",
    "#             settings = msgInterface.GetCpp2PyStruct()\n",
    "#             msgInterface.PyRecvEnd()\n",
    "\n",
    "# for j in range(1):\n",
    "#     process = Process(target=run_experiment, args=(j,))\n",
    "#     process.start()\n",
    "#     process.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ns3ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
