{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/BarbarianMatt/Code/Python/EE595ProjectAI/ns-3-dev/contrib/ai/examples/a-plus-b/use-msg-stru\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/c/Users/BarbarianMatt/Code/Python/EE595ProjectAI/ns-3-dev/contrib/ai/examples/a-plus-b/use-msg-stru\n",
    "import ns3ai_apb_py_stru as py_binding\n",
    "from ns3ai_utils import Experiment\n",
    "import sys\n",
    "import traceback\n",
    "import uuid\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "from multiprocessing import Process\n",
    "import os\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!rm /dev/shm/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # msgInterface.PyRecvBegin()\n",
    "    # temp = msgInterface.GetCpp2PyStruct()\n",
    "    # C=temp.mldThptTotal\n",
    "    # print(C)\n",
    "    # msgInterface.PyRecvEnd()\n",
    "\n",
    "    # msgInterface.PySendBegin()\n",
    "    # msgInterface.GetPy2CppStruct().c = 2\n",
    "    # msgInterface.PySendEnd()\n",
    "\n",
    "    # msgInterface.PyRecvBegin()\n",
    "    # temp = msgInterface.GetCpp2PyStruct()\n",
    "    # C=temp.mldThptTotal\n",
    "    # print(C)\n",
    "    # msgInterface.PyRecvEnd()\n",
    "\n",
    "    # for j in np.arange(1):\n",
    "    #     print(\"Starting experiment...\", flush=True)\n",
    "    #     msgInterface = exp.run(setting=setting, show_output=True)\n",
    "    #     print(\"Experiment started.\", flush=True)\n",
    "    #     start_time = time.time()\n",
    "    #     try:\n",
    "    #         for i in range(1000):\n",
    "                \n",
    "                \n",
    "    #             msgInterface.PyRecvBegin()\n",
    "\n",
    "\n",
    "    #             if msgInterface.PyGetFinished() or time.time() - start_time > 20:\n",
    "    #                 break\n",
    "\n",
    "\n",
    "    #             # print(f\"Received a: {msgInterface.GetCpp2PyStruct().a}, b: {msgInterface.GetCpp2PyStruct().b}\")\n",
    "\n",
    "    #             temp = msgInterface.GetCpp2PyStruct().a + msgInterface.GetCpp2PyStruct().b\n",
    "    #             msgInterface.PyRecvEnd()\n",
    "\n",
    "    #             msgInterface.PySendBegin()\n",
    "    #             msgInterface.GetPy2CppStruct().c = temp\n",
    "    #             msgInterface.PySendEnd()\n",
    "\n",
    "    #             # print('', flush=True)\n",
    "    #             # print(i, flush=True)\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "    #         print(\"Exception occurred: {}\".format(e), flush=True)\n",
    "    #         print(\"Traceback:\", flush=True)\n",
    "    #         traceback.print_tb(exc_traceback)\n",
    "    #         exit(1)\n",
    "    #     finally:\n",
    "    #         print(\"Finally exiting...\", flush=True)\n",
    "\n",
    "    #     # del exp\n",
    "    #     del msgInterface\n",
    "    #     gc.collect()\n",
    "    #     # subprocess.run(['rm', '-f', f'/dev/shm/My_Seg_{unique_id}'])\n",
    "    #     # time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_we_care_about = ['mldThptTotal', 'acBECwminLink1','acBECwStageLink1', 'stepNumber', 'mldMeanQueDelayTotal', 'mldMeanAccDelayTotal']\n",
    "\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    \n",
    "def load_model(model, filepath):\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()\n",
    "\n",
    "SMALL_NUMBER=float('-inf')\n",
    "INPUT_DIM=len(values_we_care_about)\n",
    "HIDDEN_DIM=32\n",
    "\n",
    "OUTPUT_DIM=2\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, INPUT_DIM, HIDDEN_DIM,OUTPUT_DIM):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.fc1 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc4 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # self.fc5 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # self.fc6 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # self.fc7 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # self.fc8 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # self.fc9 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.move_piece = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "        A=0.05\n",
    "        self.dropout = nn.Dropout(A)\n",
    "        self.dropout1 = nn.Dropout(A)\n",
    "        self.dropout2 = nn.Dropout(A)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, y, mask,valid_moves_tensor):\n",
    "    \n",
    "       \n",
    "        x = torch.relu(self.input(y))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        # x = torch.relu(self.fc5(x))\n",
    "        # x = torch.relu(self.fc6(x))\n",
    "        # x = torch.relu(self.fc7(x))\n",
    "        # x = torch.relu(self.fc8(x))\n",
    "        # x = torch.relu(self.fc9(x))\n",
    "        \n",
    "\n",
    "        move_piece = self.move_piece(x)\n",
    "\n",
    "        if mask:\n",
    "            # mask_tensor = torch.full_like(move_piece, 0)\n",
    "            mask_tensor = torch.full_like(move_piece, SMALL_NUMBER)\n",
    "            mask_tensor[valid_moves_tensor.bool()] = 0\n",
    "\n",
    "            move_piece += mask_tensor\n",
    "        return move_piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/dev/shm/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm /dev/shm/*\n",
    "\n",
    "def dton(state):\n",
    "    return np.array([0 if (key in state and (state[key] is None or np.isnan(state[key]))) else state[key] \n",
    "          for key in values_we_care_about if key in state])\n",
    "def convert(state):\n",
    "    return torch.FloatTensor(state.flatten()).unsqueeze(0)\n",
    "\n",
    "def move_nn_to_adjustments(move_nn, state):\n",
    "\n",
    "    current_acBECwminLink1=state['acBECwminLink1']\n",
    "    current_acBECwStageLink1=state['acBECwStageLink1']\n",
    "\n",
    "    if move_nn == 0:\n",
    "        # current_acBECwStageLink1 = current_acBECwStageLink1//2\n",
    "        current_acBECwminLink1 = current_acBECwminLink1 //2 \n",
    "    elif move_nn == 1:\n",
    "        # current_acBECwStageLink1 = current_acBECwStageLink1 * 2\n",
    "        current_acBECwminLink1 = current_acBECwminLink1 * 2\n",
    "    # elif move_nn == 2:\n",
    "    #     current_acBECwStageLink1 = current_acBECwStageLink1//2\n",
    "    #     current_acBECwminLink1 = acBECwminLink1\n",
    "    # elif move_nn == 3:\n",
    "    #     current_acBECwStageLink1 = current_acBECwStageLink1 * 2\n",
    "    #     current_acBECwminLink1 +=1\n",
    "\n",
    "    adjustments= {\n",
    "        'acBECwminLink1' : current_acBECwminLink1,\n",
    "        'acBECwStageLink1': current_acBECwStageLink1,\n",
    "    }\n",
    "\n",
    "    return adjustments\n",
    "\n",
    "class SimpleAgent:\n",
    "    def __init__(self, filepath):\n",
    "        self.dqn=DQN(INPUT_DIM, HIDDEN_DIM,OUTPUT_DIM)\n",
    "        self.dqn.load_state_dict(torch.load(filepath))\n",
    "        self.type = 'DQN'\n",
    "\n",
    "    def decide(self, state, valid_moves):\n",
    "\n",
    "        self.dqn.eval()\n",
    "        state_c=convert(dton(state))\n",
    "        valid_moves_c=convert(valid_moves)\n",
    "        with torch.no_grad():\n",
    "            # move_piece_q  = self.dqn(state,True,convert(self.game.vector_valid_moves(self.unique_id)))\n",
    "            move_piece_q  = self.dqn(state_c,True, valid_moves_c)\n",
    "            num_moves_choices=1\n",
    "            move_nn=torch.argsort(move_piece_q[0], descending=True)[np.random.choice(num_moves_choices)].item()\n",
    "\n",
    "            return move_nn\n",
    "\n",
    "class Environnment:\n",
    "    def __init__(self, step_size = 10, max_steps=100, mldPerNodeLambda=1e-5):\n",
    "\n",
    "        self.step_size = step_size\n",
    "        self.max_steps=max_steps\n",
    "        self.mldPerNodeLambda= mldPerNodeLambda\n",
    "        self.done_simulation=False\n",
    "        self.end_experiment = False\n",
    "\n",
    "        self.unique_id = str(uuid.uuid4())\n",
    "\n",
    "        self.filepath = '/mnt/c/Users/BarbarianMatt/Code/Python/EE595ProjectAI/ns-3-dev'\n",
    "\n",
    "        setting = { \"num_env\": 1, \n",
    "                    \"m_segmentName\":f\"My_Seg_{self.unique_id}\",\n",
    "                    \"m_cpp2pyMsgName\":f\"My_Cpp_to_Python_Msg_{self.unique_id}\",\n",
    "                    \"m_py2cppMsgName\":f\"My_Python_to_Cpp_Msg_{self.unique_id}\",\n",
    "                    \"m_lockableName\":f\"My_Lockable_{self.unique_id}\"\n",
    "                    }\n",
    "\n",
    "        self.exp = Experiment(\n",
    "            \"ns3ai_apb_msg_stru\",\n",
    "            self.filepath,\n",
    "            py_binding,\n",
    "            handleFinish=True,\n",
    "            segName=setting['m_segmentName'],\n",
    "            cpp2pyMsgName=setting['m_cpp2pyMsgName'],\n",
    "            py2cppMsgName=setting['m_py2cppMsgName'],\n",
    "            lockableName=setting['m_lockableName']\n",
    "        )\n",
    "\n",
    "        result = subprocess.run(['ls', '/dev/shm'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "        # Print the output (list of shared memory segments)\n",
    "        if result.returncode == 0:\n",
    "            print(\"Shared memory segments:\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "\n",
    "        print(\"Starting experiment...\", flush=True)\n",
    "        self.msgInterface = self.exp.run(setting=setting, show_output=True)\n",
    "        print(\"Experiment started.\", flush=True)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def sendReceive(self):\n",
    "        self.msgInterface.PySendBegin()\n",
    "        msg = self.msgInterface.GetPy2CppStruct()\n",
    "        msg.done_simulation = self.done_simulation\n",
    "        msg.end_experiment = self.end_experiment\n",
    "        for key in self.parameters:\n",
    "            # msg[key] = self.parameters[key]\n",
    "            setattr(msg, key, self.parameters[key])\n",
    "        self.msgInterface.PySendEnd()\n",
    "\n",
    "        self.msgInterface.PyRecvBegin()\n",
    "        self.state_cc = self.msgInterface.GetCpp2PyStruct()\n",
    "        self.msgInterface.PyRecvEnd()\n",
    "        self.update_state()\n",
    "\n",
    "        # print(f'Test: {self.state_cc.simulationTime}')\n",
    "\n",
    "    def update_state(self):\n",
    "        d=dir(self.state_cc)\n",
    "        state={}\n",
    "        for attribute in d:\n",
    "            if not attribute[:2] == '__':\n",
    "                state[attribute] = getattr(self.state_cc,attribute)\n",
    "        self.state = state\n",
    "        # print(state)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.parameters = {\n",
    "            'acBECwminLink1': 16,\n",
    "            'acBECwStageLink1': 6,\n",
    "            'simulationTime': self.step_size,\n",
    "            'mldPerNodeLambda': self.mldPerNodeLambda,\n",
    "            'totalSteps': self.max_steps,\n",
    "        }\n",
    "        self.num_steps=0\n",
    "\n",
    "        self.done_simulation=True \n",
    "        self.sendReceive()\n",
    "        self.done_simulation=False\n",
    "\n",
    "        return self.get_state()\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        # if self.num_steps>= self.max_steps:\n",
    "        #     self.done_simulation=True\n",
    "        self.sendReceive()\n",
    "        self.num_steps +=1\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def update_environment(self, move_nn):\n",
    "\n",
    "        action = move_nn_to_adjustments(move_nn, self.get_state())\n",
    "\n",
    "        # print('action: ', action)\n",
    "\n",
    "        for key in action:\n",
    "            adjustment = action[key]\n",
    "            self.parameters[key]=adjustment\n",
    "        \n",
    "        self.step()\n",
    "\n",
    "        reward = self.calc_reward()\n",
    "\n",
    "        \n",
    "        done = self.num_steps>self.max_steps\n",
    "\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "\n",
    "\n",
    "    def calc_reward(self):\n",
    "        reward = self.state['mldThptTotal']/10\n",
    "        return reward\n",
    "\n",
    "    def get_valid_moves(self):\n",
    "        valid_moves = np.array([1,1])\n",
    "\n",
    "        if self.state['acBECwminLink1'] <=1:\n",
    "            valid_moves[0]=0\n",
    "        \n",
    "        elif self.state['acBECwminLink1'] >=1e6:\n",
    "            valid_moves[1]=0\n",
    "\n",
    "        # if self.state['acBECwStageLink1']<=1:\n",
    "        #     valid_moves[0]=0\n",
    "        #     valid_moves[2]=0\n",
    "        \n",
    "        # elif self.state['acBECwStageLink1']>=10:\n",
    "        #     valid_moves[1]=0\n",
    "        #     valid_moves[3]=0\n",
    "        \n",
    "        # if self.state['acBECwminLink1']<=1:\n",
    "        #     valid_moves[0]=0\n",
    "        #     valid_moves[1]=0\n",
    "        \n",
    "        # elif self.state['acBECwminLink1']>=1e8:\n",
    "        #     valid_moves[2]=0\n",
    "        #     valid_moves[3]=0\n",
    "        \n",
    "        return valid_moves\n",
    "    def end(self):\n",
    "        self.end_experiment=True \n",
    "        self.sendReceive()\n",
    "        self.end_experiment=False\n",
    "\n",
    "def run():\n",
    "    x = Environnment(mldPerNodeLambda=0.01, step_size=0.1)\n",
    "    try:\n",
    "        i = 0\n",
    "        x.parameters['acBECwminLink1']=16\n",
    "        # x.parameters['acBECwStageLink1']=2\n",
    "        # print(x.get_state()['mldThptTotal'], i, flush=True)\n",
    "        for i in np.arange(1, 16):\n",
    "            x.step()\n",
    "            # x.step()\n",
    "            print(x.get_state()['mldThptTotal'], i, flush=True)\n",
    "            print(x.get_state())\n",
    "            # x.reset()\n",
    "            # print(x.get_state()['mldThptTotal'], i, flush=True)\n",
    "        \n",
    "        # x.step()\n",
    "        # # x.parameters['acBECwminLink1'] = 32\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "        # x.step()\n",
    "        # print(x.get_state()['mldThptTotal'])\n",
    "\n",
    "    finally:\n",
    "        x.end()\n",
    "\n",
    "\n",
    "# process = Process(target=run)\n",
    "# process.start()\n",
    "# process.join()\n",
    "# del process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.size=len(self.buffer)\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(args)\n",
    "        self.size=len(self.buffer)\n",
    "    # random sampling\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        indices = np.random.choice(len(self.buffer), size=batch_size, replace=False)\n",
    "        samples=[self.buffer[i] for i in indices]\n",
    "        self.buffer = deque([self.buffer[i] for i in range(len(self.buffer)) if i not in indices], maxlen=self.capacity)\n",
    "        # samples = [self.buffer.popleft() for _ in range(batch_size)]\n",
    "        self.size=len(self.buffer)\n",
    "        return samples\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done', 'valid_moves', 'next_valid_moves'))\n",
    "\n",
    "def train_dqn(env, dqn, target_dqn, episodes=1000, batch_size=64, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01,lr_start=1e-4, lr_end=1e-4, eval_every=100, games_per_eval=100):\n",
    "    episodes=int(episodes)\n",
    "    optimizer = optim.Adam(dqn.parameters(), lr_start)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    lr_decay= (lr_end / lr_start) ** (1 / episodes)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=lr_decay)\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    epsilon_decay = (epsilon_end / epsilon_start) ** (1 / (0.9*episodes))\n",
    "    optimizer.optimizations=0\n",
    "    optimizer.optimizations_cyclic=0\n",
    "\n",
    "    replay_buffer=ReplayBuffer(3*batch_size)\n",
    "\n",
    "    total_rewards_list=[]\n",
    "\n",
    "    # filepath='./dqn_new_model.pth'\n",
    "    # load_model(dqn, filepath)\n",
    "    # load_model(target_dqn, filepath)\n",
    "\n",
    "    print(\"correct one good job\")\n",
    "    def optimize_model(replay_buffer, model, target_model,optimizer):\n",
    "        total_loss=0\n",
    "        model.train()\n",
    "        while replay_buffer.size >= batch_size:\n",
    "            optimizer.optimizations+=1\n",
    "            optimizer.optimizations_cyclic+=batch_size\n",
    "            transitions = replay_buffer.sample(batch_size) # state, move, reward, resulting_state, done, repeat\n",
    "            batch = Transition(*zip(*transitions)) # seperates into a more usable structure\n",
    "\n",
    "            # print('batch: ',batch)\n",
    "            # print(batch.action)\n",
    "\n",
    "            # seperate batch into variables\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.tensor(batch.action, dtype=torch.int64) #change to int8\n",
    "            reward_batch = torch.tensor(batch.reward, dtype=torch.float32)\n",
    "            next_state_batch = torch.cat(batch.next_state)\n",
    "            done_batch = torch.tensor(batch.done, dtype=torch.float32)\n",
    "            valid_moves_batch = torch.cat(batch.valid_moves)\n",
    "            next_valid_moves_batch = torch.cat(batch.next_valid_moves)\n",
    "            # print(next_state_batch[0])\n",
    "            # print(next_state_batch[1])\n",
    "            # print()\n",
    "            \n",
    "\n",
    "            q_values = model(state_batch,True, valid_moves_batch) # the models responses to different states, masked for legality\n",
    "            state_action_values = q_values.gather(1, action_batch.view(-1, 1)).squeeze() # q_values of the actions it actually took\n",
    "\n",
    "            next_q_values = model(next_state_batch, True,next_valid_moves_batch) # the models responses to the resulting states as the next player\n",
    "            next_actions = next_q_values.max(1)[1] # what that player likely would have done\n",
    "            next_target_q_values = target_model(next_state_batch, True,next_valid_moves_batch) # target model which is updated less frequently for stability\n",
    "            next_state_values = next_target_q_values.gather(1, next_actions.unsqueeze(1)).squeeze() # q_values of the other player likely to result from your actions\n",
    "            \n",
    "            # print('reward batch: ',reward_batch)\n",
    "            # print('action batch: ',action_batch.view(-1, 1))\n",
    "            # print('next_actions: ',next_actions)\n",
    "            # print('state batch: ',state_batch)\n",
    "            # print('next state batch: ',next_state_batch)\n",
    "            # print('state_action_values: ',state_action_values)\n",
    "            # print('q_values',q_values)\n",
    "            # print('next_target_q_values',next_target_q_values)\n",
    "            # print('next_state_values',next_state_values)\n",
    "            # print('done batch',done_batch)\n",
    "            # print('valid_moves batch',valid_moves_batch)\n",
    "            # print('newt_valid_moves batch',next_valid_moves_batch)\n",
    "\n",
    "            expected_state_action_values = reward_batch + gamma * (next_state_values * (1 - done_batch)) # what the reward is from your actions\n",
    "\n",
    "            # print(expected_state_action_values)\n",
    "            \n",
    "            base_loss =loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "            # wild_jack_mask = (action_batch >= 96) & (action_batch < 192)  # Identify wild jack moves\n",
    "            # proper_use_mask = reward_batch > 0  # Identify moves that resulted in a positive reward\n",
    "\n",
    "            # # If the wild jack was used but no positive reward was obtained, increase the loss\n",
    "            # wild_jack_loss_weight = torch.ones_like(base_loss)  # Initialize loss weights to 1 (no change)\n",
    "            # wild_jack_loss_weight[wild_jack_mask & ~proper_use_mask] = 1.5  # Increase loss for wasted wild jacks\n",
    "            # wild_jack_loss_weight[wild_jack_mask & proper_use_mask] = 0.8   # Decrease loss for good use of wild jacks\n",
    "\n",
    "            # # Apply weighted loss\n",
    "            # loss = base_loss * wild_jack_loss_weight\n",
    "            loss= base_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 50.0)\n",
    "            # for param in model.parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         print(param.grad.norm().item())\n",
    "            optimizer.step()\n",
    "            total_loss+=loss.item()\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def select_epsilon_greedy_action(env, model, state, epsilon, valid_moves):\n",
    "        rand=np.random.rand()\n",
    "        # print('state: ', state, state.shape)\n",
    "        if rand<epsilon:\n",
    "\n",
    "            valid=np.where(valid_moves.numpy()[0]==1)[0]\n",
    "\n",
    "            move =np.random.choice(valid)\n",
    "            return [move]\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # print('Choose Move')\n",
    "                model.eval()\n",
    "                q_values = model(state,True, valid_moves)\n",
    "                # print('After Choose Move')\n",
    "                # print('valid_moves: ',q_values[0].cpu().numpy())\n",
    "                # print('valid_moves: ',np.isfinite(q_values[0].cpu().numpy()))\n",
    "                valid_num_moves=np.sum(np.array(np.isfinite(q_values[0].cpu().numpy())))\n",
    "                moves_to_pick_from=min(valid_num_moves,1)\n",
    "\n",
    "\n",
    "                if moves_to_pick_from > 0:\n",
    "                    return [torch.argsort(q, descending=True)[np.random.choice(moves_to_pick_from)].item() for q in q_values]\n",
    "                else:\n",
    "                    print(state)\n",
    "                    print(\"Error No Valid Moves\", flush=True)\n",
    "                    return []\n",
    "    \n",
    "    total_losses = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # print('episode: ', episode, flush=True)\n",
    "        state = convert(dton(env.reset()))\n",
    "        # print('Hello', flush=True)\n",
    "        valid_moves = convert(env.get_valid_moves())\n",
    "        total_rewards = 0\n",
    "        episode_q_values = []\n",
    "        done = False\n",
    "        moves=0\n",
    "        \n",
    "        while not done:\n",
    "            # print('Move Number: ', moves)\n",
    "            if sum(valid_moves[0])>0:\n",
    "                # print('Select Move')\n",
    "                action = select_epsilon_greedy_action(env, dqn, state, epsilon,valid_moves)\n",
    "                # print('Before Update')\n",
    "                next_state, reward, done = env.update_environment(action[0])\n",
    "                # print('After Update')\n",
    "                if episodes-episode<=2:\n",
    "\n",
    "                    print(np.array(action),np.array(dton(next_state)), reward, done)\n",
    "                next_state=convert(dton(next_state))\n",
    "                next_valid_moves=convert(env.get_valid_moves())\n",
    "                if not done and sum(next_valid_moves[0])>0:\n",
    "                    replay_buffer.push(state, action, reward, next_state, done, valid_moves,next_valid_moves)\n",
    "                else:\n",
    "                    replay_buffer.push(state, action, reward, state, True, valid_moves,valid_moves)\n",
    "                    done=True\n",
    "                state = next_state\n",
    "                valid_moves=next_valid_moves\n",
    "                total_rewards+=reward\n",
    "                # print('total_rewards: ',total_rewards)\n",
    "                moves+=1\n",
    "            else:\n",
    "                print('no valid moves backup procedure', flush=True)\n",
    "                print(state)\n",
    "                state,action,reward,next_state,done,valid_moves,next_valid_moves=replay_buffer.buffer.popleft()\n",
    "                replay_buffer.push(state, action, 0, state, True, valid_moves,valid_moves)\n",
    "                done=True\n",
    "            # print('End of Move')\n",
    "            \n",
    "        # print('Hello 2!', flush=True)\n",
    "        losses = optimize_model(replay_buffer, dqn, target_dqn, optimizer)\n",
    "        # print('losses: ', losses)\n",
    "        if losses > 0:\n",
    "            total_losses = losses\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr=scheduler.get_last_lr()[0]\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        model_filepath=\"/mnt/c/Users/BarbarianMatt/Code/Python/EE595ProjectAI/ns-3-dev/contrib/ai/examples/a-plus-b/use-msg-stru/dqn_new_model.pth\"\n",
    "        if episode % (max(episodes//10,1)) == 0 and episode>max(episodes//10,1):\n",
    "            print(\"saved over!\", flush=True)\n",
    "            save_model(dqn, model_filepath)\n",
    "        \n",
    "        if optimizer.optimizations_cyclic > 10:\n",
    "            # print('target dqn updated')\n",
    "            target_dqn.load_state_dict(dqn.state_dict())\n",
    "            optimizer.optimizations_cyclic=0\n",
    "        \n",
    "        strin=f'Episode {episode + 1}/{episodes}, Epsilon: {epsilon:.3f}, Lr: {current_lr:.3e} '\n",
    "\n",
    "        strin+= f'Rewards: {total_rewards:.4f}, Losses: {total_losses:.3f}, '\n",
    "       \n",
    "        strin+=f'Total Moves: {moves}, '\n",
    "        strin+=f'Total Optimizations: {optimizer.optimizations}'\n",
    "        print(strin, flush=True)\n",
    "\n",
    "        total_rewards_list.append(total_rewards)\n",
    "\n",
    "    env.end()\n",
    "    print(\"Training complete, saving final model.\", flush=True)\n",
    "    save_model(dqn, model_filepath)\n",
    "\n",
    "\n",
    "\n",
    "    return total_rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/dev/shm/*': No such file or directory\n",
      "Experiment Created!\n",
      "ns3ai_utils: Experiment initialized\n",
      "Shared memory segments:\n",
      "My_Seg_105ec52d-ccf0-4614-8276-5725dce6b504\n",
      "\n",
      "Starting experiment...\n",
      "Experiment started.\n",
      "correct one good job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barbarianmatt/miniconda3/envs/ns3ai_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/60, Epsilon: 0.954, Lr: 9.261e-03 Rewards: 0.2254, Losses: 0.000, Total Moves: 51, Total Optimizations: 0\n",
      "Episode 2/60, Epsilon: 0.911, Lr: 8.577e-03 Rewards: 0.5935, Losses: 0.415, Total Moves: 51, Total Optimizations: 1\n",
      "Episode 3/60, Epsilon: 0.869, Lr: 7.943e-03 Rewards: 0.9888, Losses: 0.415, Total Moves: 51, Total Optimizations: 1\n",
      "Episode 4/60, Epsilon: 0.829, Lr: 7.356e-03 Rewards: 0.3710, Losses: 0.087, Total Moves: 51, Total Optimizations: 2\n",
      "Episode 5/60, Epsilon: 0.791, Lr: 6.813e-03 Rewards: 0.1786, Losses: 0.087, Total Moves: 51, Total Optimizations: 2\n",
      "Episode 6/60, Epsilon: 0.755, Lr: 6.310e-03 Rewards: 0.1524, Losses: 0.022, Total Moves: 51, Total Optimizations: 3\n",
      "Episode 7/60, Epsilon: 0.721, Lr: 5.843e-03 Rewards: 0.6811, Losses: 0.022, Total Moves: 51, Total Optimizations: 3\n",
      "Episode 8/60, Epsilon: 0.688, Lr: 5.412e-03 Rewards: 0.2412, Losses: 3992043.250, Total Moves: 51, Total Optimizations: 4\n",
      "Episode 9/60, Epsilon: 0.656, Lr: 5.012e-03 Rewards: 0.8232, Losses: 3992043.250, Total Moves: 51, Total Optimizations: 4\n",
      "Episode 10/60, Epsilon: 0.626, Lr: 4.642e-03 Rewards: 1.5192, Losses: 12198232.000, Total Moves: 51, Total Optimizations: 5\n",
      "Episode 11/60, Epsilon: 0.598, Lr: 4.299e-03 Rewards: 0.9214, Losses: 12198232.000, Total Moves: 51, Total Optimizations: 5\n",
      "Episode 12/60, Epsilon: 0.570, Lr: 3.981e-03 Rewards: 0.2412, Losses: 3293403.750, Total Moves: 51, Total Optimizations: 6\n",
      "saved over!\n",
      "Episode 13/60, Epsilon: 0.544, Lr: 3.687e-03 Rewards: 1.8379, Losses: 3293403.750, Total Moves: 51, Total Optimizations: 6\n",
      "Episode 14/60, Epsilon: 0.520, Lr: 3.415e-03 Rewards: 2.2068, Losses: 119019.391, Total Moves: 51, Total Optimizations: 7\n",
      "Episode 15/60, Epsilon: 0.496, Lr: 3.162e-03 Rewards: 0.4793, Losses: 119019.391, Total Moves: 51, Total Optimizations: 7\n",
      "Episode 16/60, Epsilon: 0.473, Lr: 2.929e-03 Rewards: 0.3046, Losses: 5978.192, Total Moves: 51, Total Optimizations: 8\n",
      "Episode 17/60, Epsilon: 0.452, Lr: 2.712e-03 Rewards: 0.5467, Losses: 5978.192, Total Moves: 51, Total Optimizations: 8\n",
      "Episode 18/60, Epsilon: 0.431, Lr: 2.512e-03 Rewards: 0.3780, Losses: 0.026, Total Moves: 51, Total Optimizations: 9\n",
      "saved over!\n",
      "Episode 19/60, Epsilon: 0.411, Lr: 2.326e-03 Rewards: 0.4781, Losses: 0.026, Total Moves: 51, Total Optimizations: 9\n",
      "Episode 20/60, Epsilon: 0.392, Lr: 2.154e-03 Rewards: 0.2993, Losses: 0.024, Total Moves: 51, Total Optimizations: 10\n",
      "Episode 21/60, Epsilon: 0.374, Lr: 1.995e-03 Rewards: 0.2414, Losses: 0.024, Total Moves: 51, Total Optimizations: 10\n",
      "Episode 22/60, Epsilon: 0.357, Lr: 1.848e-03 Rewards: 0.2614, Losses: 0.026, Total Moves: 51, Total Optimizations: 11\n",
      "Episode 23/60, Epsilon: 0.341, Lr: 1.711e-03 Rewards: 0.4224, Losses: 0.026, Total Moves: 51, Total Optimizations: 11\n",
      "Episode 24/60, Epsilon: 0.325, Lr: 1.585e-03 Rewards: 0.4013, Losses: 0.019, Total Moves: 51, Total Optimizations: 12\n",
      "saved over!\n",
      "Episode 25/60, Epsilon: 0.311, Lr: 1.468e-03 Rewards: 0.4380, Losses: 0.019, Total Moves: 51, Total Optimizations: 12\n",
      "Episode 26/60, Epsilon: 0.296, Lr: 1.359e-03 Rewards: 0.2683, Losses: 0.025, Total Moves: 51, Total Optimizations: 13\n",
      "Episode 27/60, Epsilon: 0.283, Lr: 1.259e-03 Rewards: 0.4080, Losses: 0.025, Total Moves: 51, Total Optimizations: 13\n",
      "Episode 28/60, Epsilon: 0.270, Lr: 1.166e-03 Rewards: 0.2909, Losses: 0.020, Total Moves: 51, Total Optimizations: 14\n",
      "Episode 29/60, Epsilon: 0.258, Lr: 1.080e-03 Rewards: 0.4529, Losses: 0.020, Total Moves: 51, Total Optimizations: 14\n",
      "Episode 30/60, Epsilon: 0.246, Lr: 1.000e-03 Rewards: 0.2683, Losses: 0.017, Total Moves: 51, Total Optimizations: 15\n",
      "saved over!\n",
      "Episode 31/60, Epsilon: 0.235, Lr: 9.261e-04 Rewards: 0.4166, Losses: 0.017, Total Moves: 51, Total Optimizations: 15\n",
      "Episode 32/60, Epsilon: 0.224, Lr: 8.577e-04 Rewards: 0.3365, Losses: 0.012, Total Moves: 51, Total Optimizations: 16\n",
      "Episode 33/60, Epsilon: 0.214, Lr: 7.943e-04 Rewards: 0.4042, Losses: 0.012, Total Moves: 51, Total Optimizations: 16\n",
      "Episode 34/60, Epsilon: 0.204, Lr: 7.356e-04 Rewards: 0.4042, Losses: 0.020, Total Moves: 51, Total Optimizations: 17\n",
      "Episode 35/60, Epsilon: 0.195, Lr: 6.813e-04 Rewards: 0.6074, Losses: 0.020, Total Moves: 51, Total Optimizations: 17\n",
      "Episode 36/60, Epsilon: 0.186, Lr: 6.310e-04 Rewards: 0.4841, Losses: 0.025, Total Moves: 51, Total Optimizations: 18\n",
      "saved over!\n",
      "Episode 37/60, Epsilon: 0.177, Lr: 5.843e-04 Rewards: 0.6912, Losses: 0.025, Total Moves: 51, Total Optimizations: 18\n",
      "Episode 38/60, Epsilon: 0.169, Lr: 5.412e-04 Rewards: 0.3667, Losses: 0.040, Total Moves: 51, Total Optimizations: 19\n",
      "Episode 39/60, Epsilon: 0.161, Lr: 5.012e-04 Rewards: 0.7433, Losses: 0.040, Total Moves: 51, Total Optimizations: 19\n",
      "Episode 40/60, Epsilon: 0.154, Lr: 4.642e-04 Rewards: 0.6278, Losses: 0.032, Total Moves: 51, Total Optimizations: 20\n",
      "Episode 41/60, Epsilon: 0.147, Lr: 4.299e-04 Rewards: 0.7224, Losses: 0.032, Total Moves: 51, Total Optimizations: 20\n",
      "Episode 42/60, Epsilon: 0.140, Lr: 3.981e-04 Rewards: 0.5947, Losses: 0.045, Total Moves: 51, Total Optimizations: 21\n",
      "saved over!\n",
      "Episode 43/60, Epsilon: 0.134, Lr: 3.687e-04 Rewards: 0.7949, Losses: 0.045, Total Moves: 51, Total Optimizations: 21\n",
      "Episode 44/60, Epsilon: 0.128, Lr: 3.415e-04 Rewards: 0.5916, Losses: 0.022, Total Moves: 51, Total Optimizations: 22\n",
      "Episode 45/60, Epsilon: 0.122, Lr: 3.162e-04 Rewards: 0.7759, Losses: 0.022, Total Moves: 51, Total Optimizations: 22\n",
      "Episode 46/60, Epsilon: 0.116, Lr: 2.929e-04 Rewards: 1.2190, Losses: 0.017, Total Moves: 51, Total Optimizations: 23\n",
      "Episode 47/60, Epsilon: 0.111, Lr: 2.712e-04 Rewards: 0.8126, Losses: 0.017, Total Moves: 51, Total Optimizations: 23\n",
      "Episode 48/60, Epsilon: 0.106, Lr: 2.512e-04 Rewards: 0.5664, Losses: 0.025, Total Moves: 51, Total Optimizations: 24\n",
      "saved over!\n",
      "Episode 49/60, Epsilon: 0.101, Lr: 2.326e-04 Rewards: 0.9326, Losses: 0.025, Total Moves: 51, Total Optimizations: 24\n",
      "Episode 50/60, Epsilon: 0.096, Lr: 2.154e-04 Rewards: 0.7375, Losses: 0.024, Total Moves: 51, Total Optimizations: 25\n",
      "Episode 51/60, Epsilon: 0.092, Lr: 1.995e-04 Rewards: 1.0082, Losses: 0.024, Total Moves: 51, Total Optimizations: 26\n",
      "Episode 52/60, Epsilon: 0.088, Lr: 1.848e-04 Rewards: 0.8570, Losses: 0.024, Total Moves: 51, Total Optimizations: 26\n",
      "Episode 53/60, Epsilon: 0.084, Lr: 1.711e-04 Rewards: 0.9547, Losses: 0.037, Total Moves: 51, Total Optimizations: 27\n",
      "Episode 54/60, Epsilon: 0.080, Lr: 1.585e-04 Rewards: 0.9910, Losses: 0.037, Total Moves: 51, Total Optimizations: 27\n",
      "saved over!\n",
      "Episode 55/60, Epsilon: 0.080, Lr: 1.468e-04 Rewards: 0.9996, Losses: 0.027, Total Moves: 51, Total Optimizations: 28\n",
      "Episode 56/60, Epsilon: 0.080, Lr: 1.359e-04 Rewards: 0.8623, Losses: 0.027, Total Moves: 51, Total Optimizations: 28\n",
      "Episode 57/60, Epsilon: 0.080, Lr: 1.259e-04 Rewards: 1.1494, Losses: 0.028, Total Moves: 51, Total Optimizations: 29\n",
      "Episode 58/60, Epsilon: 0.080, Lr: 1.166e-04 Rewards: 1.1177, Losses: 0.028, Total Moves: 51, Total Optimizations: 29\n",
      "[1] [ 0. 32.  6.  2.  0.  0.] 0.0 False\n",
      "[0] [ 0. 16.  6.  3.  0.  0.] 0.0 False\n",
      "[1] [ 0. 32.  6.  4.  0.  0.] 0.0 False\n",
      "[0] [ 0. 16.  6.  5.  0.  0.] 0.0 False\n",
      "[1] [ 0.1248     32.          6.          6.          0.06260846  0.285357  ] 0.01248 False\n",
      "[0] [ 0.2616     16.          6.          7.          0.1967328   0.40456279] 0.02616 False\n",
      "[1] [ 0.5136     32.          6.          8.          3.49014019  0.70553561] 0.051359999999999996 False\n",
      "[0] [ 0.5208     16.          6.          9.          8.80712242  0.4460506 ] 0.05208 False\n",
      "[1] [ 0.5784     32.          6.         10.         17.4196466   0.38071037] 0.05784 False\n",
      "[1] [ 0.7224     64.          6.         11.         26.13401988  0.41522326] 0.07224 False\n",
      "[0] [ 0.72       32.          6.         12.         24.86666016  0.36331467] 0.072 False\n",
      "[1] [ 0.4488     64.          6.         13.         36.98710868  0.40653583] 0.044879999999999996 False\n",
      "[0] [ 0.6072     32.          6.         14.         26.49374768  0.40622688] 0.060719999999999996 False\n",
      "[1] [ 0.4992     64.          6.         15.         27.24276147  0.40175192] 0.04992 False\n",
      "[0] [ 0.5016     32.          6.         16.         22.57856893  0.669025  ] 0.05016 False\n",
      "[0] [ 0.24      16.         6.        17.        18.5388792  0.153406 ] 0.024 False\n",
      "[1] [ 0.36       32.          6.         18.         27.69076925  0.373504  ] 0.036 False\n",
      "[0] [ 0.48       16.          6.         19.         11.77637526  0.45139044] 0.048 False\n",
      "[1] [ 0.288      32.          6.         20.         28.92628817  0.38856187] 0.0288 False\n",
      "[0] [ 0.3024     16.          6.         21.         33.28282044  0.32799206] 0.03024 False\n",
      "[1] [ 0.1608     32.          6.         22.         55.91644015  0.        ] 0.01608 False\n",
      "[0] [ 0.2208     16.          6.         23.         22.19337552  0.31358043] 0.02208 False\n",
      "[1] [ 0.2088     32.          6.         24.         34.74647568  0.84624598] 0.020880000000000003 False\n",
      "[0] [ 0.2472     16.          6.         25.         33.95662355  0.38275732] 0.02472 False\n",
      "[1] [ 0.3144     32.          6.         26.         30.61954999  0.29420305] 0.03144 False\n",
      "[0] [ 0.2784     16.          6.         27.         51.57170969  0.29073621] 0.027839999999999997 False\n",
      "[1] [ 0.1128     32.          6.         28.         63.7323103   0.35534468] 0.01128 False\n",
      "[0] [ 0. 16.  6. 29.  0.  0.] 0.0 False\n",
      "[1] [4.56000000e-02 3.20000000e+01 6.00000000e+00 3.00000000e+01\n",
      " 4.60014669e+01 0.00000000e+00] 0.00456 False\n",
      "[0] [9.60000000e-03 1.60000000e+01 6.00000000e+00 3.10000000e+01\n",
      " 7.05246457e+01 0.00000000e+00] 0.0009599999999999999 False\n",
      "[1] [ 0. 32.  6. 32.  0.  0.] 0.0 False\n",
      "[0] [ 0. 16.  6. 33.  0.  0.] 0.0 False\n",
      "[1] [ 0. 32.  6. 34.  0.  0.] 0.0 False\n",
      "[0] [ 0. 16.  6. 35.  0.  0.] 0.0 False\n",
      "[1] [ 0.1416     32.          6.         36.         44.35670156  0.38753559] 0.01416 False\n",
      "[0] [ 0.4968     16.          6.         37.         42.88388002  0.23442899] 0.04968 False\n",
      "[1] [ 0.1896     32.          6.         38.         14.43810892  0.31451392] 0.018959999999999998 False\n",
      "[0] [ 0.1896     16.          6.         39.         30.76961215  0.13885316] 0.018959999999999998 False\n",
      "[1] [ 0.1752     32.          6.         40.         23.978136    0.32225479] 0.01752 False\n",
      "[0] [ 0.0984     16.          6.         41.         10.30180332  0.53245424] 0.00984 False\n",
      "[1] [ 0.0816     32.          6.         42.         43.82933506  0.        ] 0.00816 False\n",
      "[0] [ 0.0888     16.          6.         43.          4.09672605  0.61617297] 0.00888 False\n",
      "[1] [ 0.156      32.          6.         44.         13.86205132  0.45323077] 0.0156 False\n",
      "[0] [ 0.1536     16.          6.         45.         33.26724863  0.25265625] 0.015359999999999999 False\n",
      "[1] [ 0.0984     32.          6.         46.         13.74487707  0.52124878] 0.00984 False\n",
      "[0] [ 0. 16.  6. 47.  0.  0.] 0.0 False\n",
      "[1] [6.00000000e-02 3.20000000e+01 6.00000000e+00 4.80000000e+01\n",
      " 7.48914293e+01 0.00000000e+00] 0.006 False\n",
      "[0] [ 0. 16.  6. 49.  0.  0.] 0.0 False\n",
      "[1] [ 0.324      32.          6.         50.         34.33140883  0.12504593] 0.0324 False\n",
      "[0] [ 0.0816     16.          6.         51.         42.95200976  0.        ] 0.00816 False\n",
      "[1] [ 0.1824     32.          6.         52.         16.21158047  0.53887105] 0.01824 True\n",
      "Episode 59/60, Epsilon: 0.080, Lr: 1.080e-04 Rewards: 1.1285, Losses: 0.025, Total Moves: 51, Total Optimizations: 30\n",
      "[1] [ 0. 32.  6.  2.  0.  0.] 0.0 False\n",
      "[0] [ 0. 16.  6.  3.  0.  0.] 0.0 False\n",
      "[1] [ 0. 32.  6.  4.  0.  0.] 0.0 False\n",
      "[0] [ 0. 16.  6.  5.  0.  0.] 0.0 False\n",
      "[1] [ 0.12       32.          6.          6.          0.07539304  0.30567088] 0.012 False\n",
      "[0] [ 0.2616     16.          6.          7.          0.16424132  0.36757063] 0.02616 False\n",
      "[1] [ 0.5256     32.          6.          8.          3.57197374  0.74678627] 0.052559999999999996 False\n",
      "[0] [ 0.5736     16.          6.          9.         22.2770733   0.19214226] 0.05736 False\n",
      "[1] [ 0.54       32.          6.         10.         28.00902908  0.25401272] 0.054000000000000006 False\n",
      "[1] [ 0.6552     64.          6.         11.         25.4611268   0.30164469] 0.06552 False\n",
      "[0] [ 0.3552     32.          6.         12.         22.41202055  0.33060291] 0.03552 False\n",
      "[1] [ 0.3624     64.          6.         13.         37.17076558  0.3576755 ] 0.03624 False\n",
      "[0] [ 0.4008     32.          6.         14.         39.25567319  0.40904192] 0.04008 False\n",
      "[1] [ 0.5088     64.          6.         15.         35.34311756  0.37134717] 0.05088 False\n",
      "[0] [ 0.4584     32.          6.         16.         15.28539736  0.50817382] 0.04584 False\n",
      "[0] [ 0.372      16.          6.         17.         27.67245941  0.24254085] 0.0372 False\n",
      "[0] [ 0.1872      8.          6.         18.         35.50852862  0.09877436] 0.01872 False\n",
      "[1] [ 0.1416    16.         6.        19.        49.4670962  0.       ] 0.01416 False\n",
      "[1] [ 0.3648     32.          6.         20.         41.7916279   0.40671974] 0.03648 False\n",
      "[0] [ 0.1272     16.          6.         21.         40.78013372  0.        ] 0.01272 False\n",
      "[1] [ 0.1584     32.          6.         22.         36.50614424  0.        ] 0.01584 False\n",
      "[0] [ 0.0936     16.          6.         23.          5.74975692  0.43132533] 0.00936 False\n",
      "[1] [ 0.2568     32.          6.         24.         25.20791783  0.37290093] 0.025679999999999998 False\n",
      "[0] [ 0.1848     16.          6.         25.         27.42392842  0.09407792] 0.01848 False\n",
      "[0] [ 0.2568      8.          6.         26.         36.19121182  0.11808785] 0.025679999999999998 False\n",
      "[1] [ 0.3288     16.          6.         27.         35.96446321  0.36624234] 0.03288 False\n",
      "[1] [ 0.3096     32.          6.         28.         50.32779333  0.34931938] 0.030959999999999998 False\n",
      "[0] [ 0.1992     16.          6.         29.         21.61970805  0.44763373] 0.01992 False\n",
      "[1] [ 0.0816     32.          6.         30.         65.38823506  0.        ] 0.00816 False\n",
      "[0] [ 0.288      16.          6.         31.         39.33288827  0.37628667] 0.0288 False\n",
      "[1] [ 0.3096     32.          6.         32.         19.90929343  0.51875659] 0.030959999999999998 False\n",
      "[0] [ 0.228      16.          6.         33.         26.77339634  0.16663158] 0.0228 False\n",
      "[1] [ 0.144      32.          6.         34.         20.77739998  0.66268   ] 0.0144 False\n",
      "[0] [ 0.0984     16.          6.         35.         62.96698927  0.41876098] 0.00984 False\n",
      "[1] [2.64000000e-02 3.20000000e+01 6.00000000e+00 3.60000000e+01\n",
      " 7.62844191e+01 0.00000000e+00] 0.00264 False\n",
      "[0] [ 0. 16.  6. 37.  0.  0.] 0.0 False\n",
      "[1] [ 0.1344     32.          6.         38.         60.40562     0.20035357] 0.013439999999999999 False\n",
      "[1] [ 0.1296     64.          6.         39.         18.16302726  0.1698037 ] 0.01296 False\n",
      "[0] [ 0.1344     32.          6.         40.         50.19900571  0.50132143] 0.013439999999999999 False\n",
      "[0] [ 0.12       16.          6.         41.         14.84531152  0.278572  ] 0.012 False\n",
      "[1] [ 0.0816     32.          6.         42.         20.26257035  0.        ] 0.00816 False\n",
      "[0] [ 0.0672     16.          6.         43.         17.68630886  0.        ] 0.006719999999999999 False\n",
      "[1] [ 0.1632     32.          6.         44.         11.84225894  0.45023529] 0.01632 False\n",
      "[0] [ 0. 16.  6. 45.  0.  0.] 0.0 False\n",
      "[1] [ 0.276      32.          6.         46.         44.83371303  0.14261739] 0.027600000000000003 False\n",
      "[0] [ 0.4032     16.          6.         47.         34.90483527  0.45574643] 0.04032 False\n",
      "[0] [ 0.2784      8.          6.         48.         28.86105686  0.26973448] 0.027839999999999997 False\n",
      "[1] [ 0.2448     16.          6.         49.         45.39493044  0.06545298] 0.02448 False\n",
      "[1] [ 0.3672     32.          6.         50.         32.30542948  0.18383007] 0.03672 False\n",
      "[0] [ 0.2976     16.          6.         51.         23.07430023  0.45058226] 0.029759999999999998 False\n",
      "[1] [ 0.18       32.          6.         52.         21.25501195  0.172488  ] 0.018 True\n",
      "Episode 60/60, Epsilon: 0.080, Lr: 1.000e-04 Rewards: 1.1796, Losses: 0.025, Total Moves: 51, Total Optimizations: 30\n",
      "Training complete, saving final model.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m process \u001b[38;5;241m=\u001b[39m Process(target\u001b[38;5;241m=\u001b[39mrun_ai)\n\u001b[1;32m     19\u001b[0m process\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 20\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ns3ai_env/lib/python3.9/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ns3ai_env/lib/python3.9/multiprocessing/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m~/miniconda3/envs/ns3ai_env/lib/python3.9/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!rm /dev/shm/*\n",
    "np.random.seed(25)\n",
    "\n",
    "def run_ai():\n",
    "    env=Environnment(step_size=0.1,max_steps=50, mldPerNodeLambda=0.01)\n",
    "    try:\n",
    "\n",
    "        dqn = DQN(INPUT_DIM, HIDDEN_DIM,OUTPUT_DIM)\n",
    "        dqn_target = DQN(INPUT_DIM, HIDDEN_DIM,OUTPUT_DIM)\n",
    "\n",
    "\n",
    "        _=total_reward_lists=train_dqn(env, dqn, dqn_target, episodes=60, batch_size=100, gamma=0.999, epsilon_start=1, epsilon_end=0.08, lr_start=1e-2, lr_end=1e-4)\n",
    "    finally:\n",
    "        env.end()\n",
    "        del env\n",
    "    # print('Awesome')\n",
    "\n",
    "process = Process(target=run_ai)\n",
    "process.start()\n",
    "process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm /dev/shm/*\n",
    "# def run_experiment(loop_id):\n",
    "\n",
    "#     unique_id = str(uuid.uuid4())\n",
    "#     print(f\"Starting loop {loop_id}, PID: {os.getpid()}\")\n",
    "\n",
    "#     filepath = '/mnt/c/Users/BarbarianMatt/Code/Python/EE595ProjectAI/ns-3-dev'\n",
    "\n",
    "#     setting = { \"num_env\": 1, \n",
    "#                 \"m_segmentName\":f\"My_Seg_{unique_id}\",\n",
    "#                 \"m_cpp2pyMsgName\":f\"My_Cpp_to_Python_Msg_{unique_id}\",\n",
    "#                 \"m_py2cppMsgName\":f\"My_Python_to_Cpp_Msg_{unique_id}\",\n",
    "#                 \"m_lockableName\":f\"My_Lockable_{unique_id}\"\n",
    "#                 }\n",
    "\n",
    "#     exp = Experiment(\n",
    "#         \"ns3ai_apb_msg_stru\",\n",
    "#         filepath,\n",
    "#         py_binding,\n",
    "#         handleFinish=True,\n",
    "#         segName=setting['m_segmentName'],\n",
    "#         cpp2pyMsgName=setting['m_cpp2pyMsgName'],\n",
    "#         py2cppMsgName=setting['m_py2cppMsgName'],\n",
    "#         lockableName=setting['m_lockableName']\n",
    "#     )\n",
    "\n",
    "#     result = subprocess.run(['ls', '/dev/shm'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "#     # Print the output (list of shared memory segments)\n",
    "#     if result.returncode == 0:\n",
    "#         print(\"Shared memory segments:\")\n",
    "#         print(result.stdout)\n",
    "#     else:\n",
    "#         print(f\"Error: {result.stderr}\")\n",
    "\n",
    "#     print(\"Starting experiment...\", flush=True)\n",
    "#     msgInterface = exp.run(setting=setting, show_output=True)\n",
    "#     print(\"Experiment started.\", flush=True)\n",
    "\n",
    "#     end = False\n",
    "#     for j in np.arange(10):\n",
    "#         done = False\n",
    "#         for i in np.arange(20):\n",
    "#             if i == 19:\n",
    "#                 done = True\n",
    "#             if done and j == 9:\n",
    "#                 end = True\n",
    "            \n",
    "#             msgInterface.PySendBegin()\n",
    "#             temp =msgInterface.GetPy2CppStruct()\n",
    "            \n",
    "#             temp.done_simulation = done\n",
    "#             temp.end_experiment = end\n",
    "#             msgInterface.PySendEnd()\n",
    "\n",
    "#             msgInterface.PyRecvBegin()\n",
    "#             settings = msgInterface.GetCpp2PyStruct()\n",
    "#             msgInterface.PyRecvEnd()\n",
    "\n",
    "# for j in range(1):\n",
    "#     process = Process(target=run_experiment, args=(j,))\n",
    "#     process.start()\n",
    "#     process.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ns3ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
